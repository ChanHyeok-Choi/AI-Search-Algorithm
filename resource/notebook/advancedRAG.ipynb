{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"FOf5h4yEKoKO"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/psm/anaconda3/envs/anz/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import unicodedata\n","\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","import fitz  # PyMuPDF\n","import pymupdf4llm\n","\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    pipeline,\n","    BitsAndBytesConfig\n",")\n","from accelerate import Accelerator\n","\n","# Langchain 관련\n","from langchain.llms import HuggingFacePipeline\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","# from llama_index.core import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.schema.output_parser import StrOutputParser\n","\n","from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n","from langchain_community.retrievers import BM25Retriever\n","# from langchain_teddynote.retrievers import KiwiBM25Retriever\n","\n","from langchain.retrievers.document_compressors import FlashrankRerank\n","\n","\n","# psm pdf 함수 관련\n","import glob\n","import pdfplumber\n","from langchain_community.document_loaders import PyPDFLoader\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["MARKDOWN_SEPARATORS = [\n","    \"\\n#{1,6} \",\n","    \"```\\n\",\n","    \"\\n\\\\*\\\\*\\\\*+\\n\",\n","    \"\\n---+\\n\",\n","    \"\\n___+\\n\",\n","    \"\\n\\n\",\n","    \"\\n\",\n","    \" \",\n","    \"\",\n","]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from kiwipiepy import Kiwi\n","\n","kiwi = Kiwi()\n","\n","def kiwi_tokenize(text):\n","    return [token.form for token in kiwi.tokenize(text)]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def process_pdfs_psm(file_path, chunk_size=512, chunk_overlap=32):\n","    pdf_files = glob.glob(file_path)\n","    pages = []\n","    \n","    # PDF 파일에서 페이지 정보를 로드합니다.\n","    for pdf_file_path in pdf_files:\n","        loader = PyPDFLoader(pdf_file_path)\n","        pages.extend(loader.load_and_split())\n","\n","    extracted_tables = {}\n","\n","    # 각 PDF 파일에 대해 테이블 추출 작업을 수행합니다.\n","    for pdf_file_path in pdf_files:\n","        with pdfplumber.open(pdf_file_path) as pdf:\n","            for i, page in enumerate(pdf.pages):\n","                # 이미지 처리 로직 추가 가능\n","\n","                # 테이블 추출\n","                table = page.extract_table()\n","                if table:\n","                    df = pd.DataFrame(table[1:], columns=table[0])\n","                    markdown_table = df.to_markdown()\n","                    extracted_tables[(pdf_file_path, i)] = markdown_table\n","                else:\n","                    extracted_tables[(pdf_file_path, i)] = \"No table found\"\n","\n","    all_content = \"\"\n","    # 추출된 테이블을 페이지 컨텐츠에 추가하고 전체 텍스트를 하나로 합칩니다.\n","    for page in pages:\n","        pdf_file = page.metadata['source']\n","        page_number = page.metadata['page']\n","        \n","        if (pdf_file, page_number) in extracted_tables:\n","            table_content = extracted_tables[(pdf_file, page_number)]\n","            page_content = page.page_content\n","            updated_content = f\"{page_content}\\n\\nTable extracted from page {page_number}:\\n{table_content}\"\n","            page.page_content = updated_content\n","        \n","        all_content += \"\\n\\n\" + page.page_content  # 모든 페이지 컨텐츠를 하나의 문자열로 연결\n","\n","    # 전체 텍스트를 chunk로 분할\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","        add_start_index=True,\n","        strip_whitespace=True,\n","        separators=[\"\\n\\n\", \"\\n\"]  # MARKDOWN_SEPARATORS로 설정해야 하는 구체적인 값을 설정\n","    )\n","    \n","    chunks = splitter.split_text(all_content)\n","    return [Document(page_content=t) for t in chunks]\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def process_pdf_by_markdown(file_path, chunk_size=512, chunk_overlap=32):\n","    md_text = pymupdf4llm.to_markdown(file_path)\n","    \n","    headers_to_split_on = [\n","        (\"#\", \"Header 1\"),\n","        (\"##\", \"Header 2\"),\n","        (\"###\", \"Header 3\"),\n","    ]\n","\n","    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n","    md_chunks = md_header_splitter.split_text(md_text)\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n","    )\n","\n","    splits = text_splitter.split_documents(md_chunks)\n","    return splits\n","\n","def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n","    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n","    # PDF 파일 열기\n","    doc = fitz.open(file_path)\n","    text = ''\n","    # 모든 페이지의 텍스트 추출\n","    for page in doc:\n","        text += page.get_text()\n","    # 텍스트를 chunk로 분할\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","        add_start_index=True,\n","        strip_whitespace=True,\n","        separators=MARKDOWN_SEPARATORS\n","    )\n","    chunk_temp = splitter.split_text(text)\n","    # Document 객체 리스트 생성\n","    chunks = [Document(page_content=t) for t in chunk_temp]\n","    return chunks\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","\n","\n","def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-large\"):\n","    \"\"\"FAISS DB 생성\"\"\"\n","    # 임베딩 모델 설정\n","    model_kwargs = {'device': 'cuda'}\n","    encode_kwargs = {'normalize_embeddings': True}\n","    embeddings = HuggingFaceEmbeddings(\n","        model_name=model_path,\n","        model_kwargs=model_kwargs,\n","        encode_kwargs=encode_kwargs\n","    )\n","    # FAISS DB 생성 및 반환\n","    db = FAISS.from_documents(chunks, embedding=embeddings)\n","    \n","    return db\n","\n","def normalize_path(path):\n","    \"\"\"경로 유니코드 정규화\"\"\"\n","    return unicodedata.normalize('NFC', path)\n","\n","\n","def process_pdfs_from_dataframe(df, base_directory):\n","    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n","    pdf_databases = {}\n","    unique_paths = df['Source_path'].unique()\n","\n","    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n","        # 경로 정규화 및 절대 경로 생성\n","        normalized_path = normalize_path(path)\n","        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n","\n","        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n","        print(f\"Processing {pdf_title}...\")\n","\n","        # PDF 처리 및 벡터 DB 생성\n","        chunks = process_pdf(full_path)\n","        \n","        # # Markdown 형식으로 처리해서 벡터 DB 생성\n","        chunks = process_pdf_by_markdown(full_path)\n","        # chunks 생성 후 검증\n","        \n","        # chunks = process_pdfs_psm(full_path)\n","        if not chunks:\n","            raise ValueError(\"No chunks generated from the PDFs. Check if the PDFs contain readable text or tables.\")\n","\n","        db = create_vector_db(chunks)\n","        \n","        # Retriever 생성\n","        retriever = db.as_retriever(search_type=\"mmr\",\n","                                    search_kwargs={'k': 5, 'fetch_k': 10})\n","        \n","        bm25_retriever = BM25Retriever.from_documents(chunks, preprocess_func=kiwi_tokenize,\n","                                                      search_kwargs={'k': 5, 'fetch_k': 10})\n","\n","        # initialize the ensemble retriever\n","        ensemble_retriever = EnsembleRetriever(\n","            retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5],\n","        )\n","\n","        # 결과 저장\n","        pdf_databases[pdf_title] = {\n","                'db': db,\n","                'retriever': ensemble_retriever\n","        }\n","    return pdf_databases\n","\n","def documents_from_pdfs(df, base_directory):\n","    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n","    pdf_databases = {}\n","    unique_paths = df['Source_path'].unique()\n","    documents = []\n","    \n","    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n","        # 경로 정규화 및 절대 경로 생성\n","        normalized_path = normalize_path(path)\n","        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n","\n","        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n","        print(f\"Processing {pdf_title}...\")\n","\n","        # PDF 처리 및 벡터 DB 생성\n","        chunks = process_pdfs_psm(full_path)\n","        documents.extend(chunks)\n","    return documents\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing PDFs:   0%|          | 0/9 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Processing 중소벤처기업부_혁신창업사업화자금(융자)...\n"]},{"name":"stderr","output_type":"stream","text":["/home/psm/anaconda3/envs/anz/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n","  warn_deprecated(\n","Processing PDFs:  11%|█         | 1/9 [00:08<01:04,  8.02s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 보건복지부_부모급여(영아수당) 지원...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  22%|██▏       | 2/9 [00:12<00:42,  6.10s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 보건복지부_노인장기요양보험 사업운영...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  33%|███▎      | 3/9 [00:19<00:37,  6.20s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 산업통상자원부_에너지바우처...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  44%|████▍     | 4/9 [00:24<00:29,  5.86s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 국토교통부_행복주택출자...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  56%|█████▌    | 5/9 [00:29<00:21,  5.40s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 「FIS 이슈 & 포커스」 22-4호 《중앙-지방 간 재정조정제도》...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  67%|██████▋   | 6/9 [00:36<00:18,  6.21s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  78%|███████▊  | 7/9 [00:44<00:13,  6.85s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 「FIS 이슈&포커스」 22-2호 《재정성과관리제도》...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs:  89%|████████▉ | 8/9 [00:51<00:06,  6.62s/it]"]},{"name":"stdout","output_type":"stream","text":["Processing 「FIS 이슈 & 포커스」(신규) 통권 제1호 《우발부채》...\n"]},{"name":"stderr","output_type":"stream","text":["Processing PDFs: 100%|██████████| 9/9 [00:56<00:00,  6.32s/it]\n"]}],"source":["base_directory = '/home/psm/model/data' # Your Base Directory\n","df = pd.read_csv('/home/psm/model/data/test.csv')\n","# documents = documents_from_pdfs(df, base_directory)\n","pdf_databases = process_pdfs_from_dataframe(df, base_directory)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Helper function for printing docs\n","def pretty_print_docs(docs):\n","    print(\n","        f\"\\n{'-' * 100}\\n\".join(\n","            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n","        )\n","    )"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def normalize_string(s):\n","    \"\"\"유니코드 정규화\"\"\"\n","    return unicodedata.normalize('NFC', s)\n","\n","def format_docs(docs):\n","    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n","    context = \"\"\n","    for doc in docs:\n","        context += doc.page_content\n","        context += '\\n'\n","    return context"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["중소벤처기업부_혁신창업사업화자금(융자)\n","2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요?\n","\n","Document 1:\n","\n","## 직접 출자 출연 보조 융자 국고보조율(%) 융자율 (%)\n","○ ○  \n","3. 지출계획 총괄표  \n","(단위: 백만원, %)  \n","|사업명|2022년 결산|2023년|Col4|2024년|증감 (B-A) (B-A)/A|Col7|\n","|---|---|---|---|---|---|---|\n","|||당초(A)|수정|확정(B)||(B-A)/A|\n","|혁신창업사업화자 금(융자)|2,300,000|2,230,000|2,330,000|2,007,800|△222,200|△9.96|  \n","2023년 2024년  \n","2022년 증감\n","사업명  \n","결산 당초(A) 수정 확정(B) (B-A) (B-A)/A  \n","혁신창업사업화자  \n","2,300,000 2,230,000 2,330,000 2,007,800 △222,200 △9.96  \n","금(융자)\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n","## 사  업  명  \n","혁신창업사업화자금(융자) (5152-301)\n","----------------------------------------------------------------------------------------------------\n","Document 3:\n","\n","-  2014. 1 지원대상 업력기준을 7년 미만으로 확대  \n","-  2015. 1 재창업자금을 재도약지원자금(융자)의 내역사업으로 이관  \n","-  2019. 1 청년전용창업자금을 혁신창업지원자금에 통합 운영  \n","개발기술사업화자금을 내역사업으로 통합 및 일자리창출촉진자금 신규 지원  \n","-  2020. 1 미래기술육성자금, 고성장촉진자금 신규 지원  \n","-  2022. 1 미래기술육성자금 및 고성장촉진자금 사업 폐지  \n","-  2023. 1 창업기반지원과 신청 대상이 중복인 일자리창출촉진자금을 폐지  \n","-  2023. 1 성장공유형 대출방식 추가 (혁신창업사업화자금)\n","----------------------------------------------------------------------------------------------------\n","Document 4:\n","\n","## 4. 사업목적  \n","ㅇ (창업기반지원) 기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의  \n","창업을 활성화하고 고용창출 도모  \n","ㅇ (개발기술사업화) 중소기업이 보유한 우수 기술의 사장을 방지하고 개발기술의  \n","제품화·사업화를 촉진하여 기술기반 중소기업을 육성\n","----------------------------------------------------------------------------------------------------\n","Document 5:\n","\n","## 구분 프로그램 단위사업 세부사업\n","코드 5100 5152 301\n","명칭 창업환경조성 창업기업지원융자(기금) 혁신창업사업화자금(융자)  \n","2. 사업 지원 형태 및 지원율  \n","|직접|출자|출연|보조|융자|국고보조율(%)|융자율 (%)|\n","|---|---|---|---|---|---|---|\n","||||○|○|||\n","----------------------------------------------------------------------------------------------------\n","Document 6:\n","\n","## 명칭 창업 및 진흥기금 창업 및 벤처  \n","및 에너지\n"]}],"source":["q_n = 0\n","query = df['Question'][q_n]\n","# answer = df['Answer'][q_n]\n","source = df['Source'][q_n]\n","\n","normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n","retriever = normalized_keys[normalize_string(source)]['retriever']\n","\n","print(source)\n","print(query)\n","# print(answer)\n","print()\n","\n","docs = retriever.invoke(query)\n","pretty_print_docs(docs)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def setup_llm_pipeline():\n","    # 4비트 양자화 설정\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16\n","    )\n","\n","    # 모델 ID\n","    model_id = \"rtzr/ko-gemma-2-9b-it\"\n","    # model_id = \"/home/psm/model/resource/notebook/results/checkpoint-500\"\n","\n","    # 토크나이저 로드 및 설정\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    tokenizer.use_default_system_prompt = False\n","\n","    # 모델 로드 및 양자화 설정 적용\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        quantization_config=bnb_config,\n","        device_map=\"auto\",\n","        trust_remote_code=True\n","    )\n","\n","    # HuggingFacePipeline 객체 생성\n","    text_generation_pipeline = pipeline(\n","        model=model,\n","        tokenizer=tokenizer,\n","        task=\"text-generation\",\n","        # temperature=0.2,\n","        return_full_text=False,\n","        max_new_tokens=300,\n","    )\n","\n","    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","    return hf, tokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n","/home/psm/anaconda3/envs/anz/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n","  warn_deprecated(\n"]}],"source":["# LLM 파이프라인\n","llm, tokenizer = setup_llm_pipeline()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n","from langchain.retrievers.document_compressors import CrossEncoderReranker\n","\n","model = HuggingFaceCrossEncoder(model_name=\"Dongjin-kr/ko-reranker\")\n","compressor = CrossEncoderReranker(model=model, top_n=5)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'tqdm' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# DataFrame의 각 행에 대해 처리\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswering Questions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 소스 문자열 정규화\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     source \u001b[38;5;241m=\u001b[39m normalize_string(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m     question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"]}],"source":["\n","\n","\n","# 결과를 저장할 리스트 초기화\n","results = []\n","\n","# DataFrame의 각 행에 대해 처리\n","for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n","    # 소스 문자열 정규화\n","    source = normalize_string(row['Source'])\n","    question = row['Question']\n","\n","    # 정규화된 키로 데이터베이스 검색\n","    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n","    retriever = normalized_keys[source]['retriever']\n","    \n","    # Compressed revriever\n","    compression_retriever = ContextualCompressionRetriever(\n","        base_compressor=compressor, base_retriever=retriever,\n","    )\n","\n","    # RAG 체인 구성\n","    template = \"\"\"\n","    ### 다음 정보를 바탕으로 질문에 답하세요 (간결하게 답변하고 똑같은 단어는 반복하지 마세요):\n","    {context}\n","\n","    ### 질문: {question}\n","\n","    ### 답변:\n","    \"\"\"\n","    prompt = PromptTemplate.from_template(template)\n","    \n","    rag_chain = (\n","        {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n","        | prompt\n","        | llm\n","        | StrOutputParser()\n","    )\n","\n","    # 답변 추론\n","    print(f\"Question: {question}\")\n","    full_response = rag_chain.invoke(question)\n","\n","    print(f\"Answer: {full_response}\\n\")\n","\n","    # 결과 저장\n","    results.append({\n","        \"Source\": row['Source'],\n","        \"Source_path\": row['Source_path'],\n","        \"Question\": question,\n","        \"Answer\": full_response\n","    })\n","    \n","    \n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SAMPLE_ID</th>\n","      <th>Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_000</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_001</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_002</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_003</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_004</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>TRAIN_095</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>TRAIN_096</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>TRAIN_097</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>TRAIN_098</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>TRAIN_099</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 2 columns</p>\n","</div>"],"text/plain":["    SAMPLE_ID Answer\n","0   TRAIN_000       \n","1   TRAIN_001       \n","2   TRAIN_002       \n","3   TRAIN_003       \n","4   TRAIN_004       \n","..        ...    ...\n","95  TRAIN_095       \n","96  TRAIN_096       \n","97  TRAIN_097       \n","98  TRAIN_098       \n","99  TRAIN_099       \n","\n","[100 rows x 2 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["new_data = {\n","    'SAMPLE_ID': ['TRAIN_{:03d}'.format(i) for i in range(len(results))],\n","    'Answer': \"\"\n","}\n","submit_df = pd.DataFrame(new_data)\n","submit_df"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# 제출용 샘플 파일 로드\n","submit_df = pd.read_csv(\"/home/psm/model/data/sample_submission.csv\")\n","\n","# 생성된 답변을 제출 DataFrame에 추가\n","submit_df['Answer'] = [item['Answer'].split(\"### 질문\")[0].strip() for item in results]\n","submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n","\n","\n","# 결과를 CSV 파일로 저장\n","submit_df.to_csv(\"/home/psm/model/data/08166_submission.csv\", encoding='UTF-8-sig', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Helper function for printing docs\n","\n","def pretty_print_docs(docs):\n","    print(\n","        f\"\\n{'-' * 100}\\n\".join(\n","            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n","        )\n","    )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# 제출용 샘플 파일 로드\n","# submit_df = pd.read_csv(\"/home/psm/model/data/sample_submission.csv\")\n","\n","# 생성된 답변을 제출 DataFrame에 추가\n","submit_df['Answer'] = [item['Answer'].split(\"### 질문\")[0].strip() for item in results]\n","submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n","\n","# 결과를 CSV 파일로 저장\n","submit_df.to_csv(\"/home/psm/model/data/0816_train_infer_3.csv\", encoding='UTF-8-sig', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMHbQitHoRsTHhUZfj5XzIY","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
